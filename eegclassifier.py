# -*- coding: utf-8 -*-
"""eegnew.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TPLWr9SbXNgOJIuG1t_iYjwiqOHOYMuQ
"""

!pip install mne
# Install required Python libraries for EEG processing and machine learning
!pip install numpy scipy matplotlib latexify-py skfeature-chappers

from google.colab import drive
drive.mount('/content/drive')

import os
import mne
import numpy as np
import matplotlib.pyplot as plt
import warnings

# Ignore warnings for a cleaner output
warnings.filterwarnings("ignore", message=".*annotation.*")

folder_path = "/content/drive/MyDrive/SSVEP-based-EEG-signal-processing-main/SSVEP-based-EEG-signal-processing-main/Data"

def data_path(folder_path, data_format="gdf"):
    path_files = []  # Store paths of matching files
    files = []  # Store file names
    folders = []  # Store folder names

    # Walk through the directory and collect relevant files/folders
    for root, dirnames, filenames in os.walk(folder_path):
        for filename in filenames:
            if filename.endswith(f".{data_format}"):
                full_path = os.path.join(root, filename)
                path_files.append(full_path)
                files.append(filename)
        folders.extend(dirnames)

    return path_files, files, folders

# Get the paths of all GDF files
path_files, files, folders = data_path(folder_path, data_format="gdf")

# Print the collected file paths
print("Found files:", files)
print("Full paths:", path_files)

# Ensure the path_files variable contains GDF file paths
print(f"Using GDF file: {path_files[0]}")

# Read the GDF file into a raw MNE object
raw = mne.io.read_raw_gdf(path_files[0], verbose=0)

# Extract the channel names from the raw data
channels_name = raw.ch_names

# Get the EEG data and transpose it to have channels as rows and samples as columns
data = 1e6 * raw.get_data().T  # Convert to microvolts if necessary

# Get the sampling frequency of the EEG data
fs = raw.info['sfreq']

# Extract labels from annotations
labels = raw.annotations.description

# Get the events and their corresponding indices
events, event_ind = mne.events_from_annotations(raw, verbose=0)

# Print all the relevant information
print(f"Data: {data.shape} \n")
print(f"Channels Name: {channels_name} \n")
print(f"Labels: {labels} \n")
print(f"Events: {events} \n")
print(f"Event Indices: {event_ind} \n")

import pandas as pd
# Create a DataFrame for the EEG data with channel names as columns
df = pd.DataFrame(data, columns=channels_name)
# Add the labels to the DataFrame as a new column
df['Labels'] = labels
# Display the first few rows of the DataFrame
print(df.head())

"""Filtering/ feature Extraction / Preprocessing"""

#Import required libraries
import numpy as np
import mne
import warnings
import sys

sys.path.append('/content/drive/MyDrive/SSVEP-based-EEG-signal-processing-main/SSVEP-based-EEG-signal-processing-main/Code/Python/Functions')

# Import functions from the .py files in the Functions folder
from Filtering import filtering

# Step 4: Define your parameters for filtering
trial = 8            # Define trial number (trial 1 in Python index starts from 0)
order = 3            # Define filter order
f_low = 0.05         # Define lower cutoff frequency for the bandpass filter (Hz)
f_high = 100         # Define upper cutoff frequency for the bandpass filter (Hz)
notch_freq = 50      # Define frequency to be removed from the signal for notch filter (Hz)
quality_factor = 20  # Define quality factor for the notch filter
notch_filter = "on"  # on or off
filter_active = "on" # on or off
design_method = "IIR" # IIR or FIR
type_filter = "bandpass"  # low, high, bandpass, or bandstop
freq_stim = 13       # Define stimulation frequency


# Step 5: Apply bandpass filtering to the EEG data
filtered_data = filtering(data1, f_low, f_high, order, fs, notch_freq, quality_factor,
                          filter_active, notch_filter, type_filter, design_method)

# Print the shape of the filtered data to verify
print(f"Filtered data shape: {filtered_data.shape}")

"""CAR Filter (Common average reference)"""

#Import the CAR filter function
from Common_average_reference import car

# Step 3: Apply CAR filter to the data
# filtered_data shape: (1280, 8, 160)
data_car = car(filtered_data, reference_channel=None)  # Use all channels for average reference

# Step 4: Verify the shape of CAR-filtered data
print(f"CAR-filtered data shape: {data_car.shape}")

# Step 5: Extract the trial-specific data if needed
trial = 0  # Define trial number (0-indexed)
trial_data_car = data_car[:, :, trial]  # Extract data for the selected trial

# Step 6: Verify the trial data shape
print(f"CAR-filtered data for Trial {trial + 1}: {trial_data_car.shape}")

"""CCA"""

# Import functions
from Filtering import filtering
from Common_average_reference import car
from CCA import cca

# ------------------------------------ Step 2: Load and Combine Data ----------------------------------------
# Assuming data1, data2, and data3 are already loaded
data_total = np.concatenate((data1, data2, data3), axis=2)

# Generate labels for each dataset (0, 1, 2)
labels = np.concatenate((np.full(data1.shape[-1], 0),
                         np.full(data2.shape[-1], 1),
                         np.full(data3.shape[-1], 2)))

# ------------------------------------ Step 3: Filter the Data ----------------------------------------
order = 4
f_low = 0.05
f_high = 100
notch_freq = 50
quality_factor = 20
notch_filter = "on"
filter_active = "off"
type_filter = "bandpass"

filtered_data = filtering(data_total, f_low, f_high, order, fs,
                          notch_freq, quality_factor, filter_active,
                          notch_filter, type_filter)

print(f"Filtered Data Shape: {filtered_data.shape}")

# ----------------------------------- Step 4: Apply CAR Filter ----------------------------------------
data_car = car(filtered_data)
print(f"CAR-Filtered Data Shape: {data_car.shape}")

# ----------------------------------- Step 5: Perform CCA Classification ----------------------------------------
num_channel = [0, 1, 2]   # List of channels to use
num_harmonic = 4          # Number of harmonics
f_stim = [13, 21, 17]     # Frequencies used for stimulation

# Use your CCA function to classify the EEG signals
predict_label = cca(data_car, fs, f_stim, num_channel, num_harmonic)

print(predict_label)

# ------------------------------------ Step 6: Calculate Accuracy ----------------------------------------
accuracy = np.sum(labels == predict_label) / len(predict_label) * 100
print(f"Classification Accuracy: {accuracy:.2f}%")

"""Feature Extraction using CCA"""

#Import functions from the files
import Filtering
import Common_average_reference
import CCA_Feature_Extraction
import numpy as np

#Combine all datasets
data_total = np.concatenate((data1, data2, data3), axis=2)
labels = np.concatenate((np.full(data1.shape[-1], 0),
                         np.full(data2.shape[-1], 1),
                         np.full(data3.shape[-1], 2)))

# Define filtering parameters
order = 3                # Define filter order
notch_freq = 50          # Define frequency to be removed from the signal for notch filter (Hz)
quality_factor = 20      # Define quality factor for the notch filter
subbands = [[12, 16, 20], [14, 18, 22]]
f_low = np.min(subbands) - 1  # Define lower cutoff frequency for the bandpass filter (Hz)
f_high = np.max(subbands) + 1  # Define upper cutoff frequency for the bandpass filter (Hz)
notch_filter = "on"       # on or off
filter_active = "on"      # on or off
type_filter = "bandpass"  # low, high, bandpass, or bandstop

# Apply notch filter to the EEG data
filtered_data = Filtering.filtering(data_total, f_low, f_high, order, fs,
                                     notch_freq, quality_factor,
                                     filter_active, notch_filter, type_filter)

# Perform Common Average Reference (CAR)
data_car = Common_average_reference.car(filtered_data)

# Define parameters for feature extraction
num_channel = [0, 1, 2]   # List of channels to use
num_harmonic = 2          # Number of harmonics
f_stim = [13, 21, 17]     # Frequencies stimulation

title = f"Feature Extraction using CCA"

# Perform CCA feature extraction
features_extraction = CCA_Feature_Extraction.cca_feature_extraction(data_car, fs, f_stim, num_channel, num_harmonic)

# Print or visualize the extracted features
print("Extracted Features: ", features_extraction)
print("Extracted Features Shape: ", features_extraction)

"""Feature Selection"""

# Import the feature selection function from the uploaded file
from Feature_selections import feature_selecions

# Define parameters for feature selection
num_features = 4
n_neighbors_MI = 5                 # Number of neighbors to consider for mutual information calculation.
L1_Parameter = 0.1                 # Parameter value for L1 regularization.
threshold_var = 0.001              # The threshold used for variance thresholding.
type_feature_selection = "anova"    # Options: var, anova, mi, ufs, rfe, rf, l1fs, tfs, fs, ffs, bfs
title = f"Feature selection using {type_feature_selection}"

# Perform feature selection
features = feature_selecions(features_extraction, labels, num_features, threshold_var,
                              n_neighbors_MI, L1_Parameter, type_feature_selection)

# Display the selected features
print(f"Selected features shape: {features.shape}")

"""Classification Models"""

# Step 1: Import necessary libraries
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming `features_extraction` and `labels` are already available from the previous steps

# Step 2: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    features_extraction, labels, test_size=0.2, random_state=42
)
# Step 3: Standardize the features (important for models like SVM)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# Support Vector Machine
svm_model = SVC(kernel='linear', probability=True, random_state=42)
svm_model.fit(X_train, y_train)

# Step 5: Evaluate models on the test set
models = {#"Logistic Regression": logreg,
           "SVM": svm_model
          #  , "Random Forest": rf_model
          }

for name, model in models.items():
    y_pred = model.predict(X_test)
    print(f"--- {name} ---")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
    print(classification_report(y_test, y_pred))

# Step 6: Define action mapping based on predictions
def action_mapping(prediction):
    actions = {
        0: "13",
        1: "21",
        2: "17"
    }
    return actions.get(prediction, "Unknown Action")

print(f"Xtest = {X_test.shape}")
print(f"Xtrain = {X_train.shape}")


# Step 7: Test the model with some example data
for i in range (features.shape[1]):
    sample = X_test[i].reshape(1, -1)  # Example data point
    predicted_label = rf_model.predict(sample)[0]
    predicted_action = action_mapping(predicted_label)

    print(f"Predicted Action: {predicted_action}")

"""## DEPLOYMENT"""

import numpy as np

# Load the .npy file
data1 = np.load('/content/drive/MyDrive/SSVEP-based-EEG-signal-processing-main/SSVEP-based-EEG-signal-processing-main/trials/13hz.npy') [:,:,0]
data2 = np.load('/content/drive/MyDrive/SSVEP-based-EEG-signal-processing-main/SSVEP-based-EEG-signal-processing-main/trials/21hz.npy')[:,:,0]
data3 = np.load('/content/drive/MyDrive/SSVEP-based-EEG-signal-processing-main/SSVEP-based-EEG-signal-processing-main/trials/17hz.npy')[:,:,0]

# Example event information (you should have these from the old data)
event_ind = {'32769': 1, '32770': 2, '32779': 3, '32780': 4, '33024': 5,
             '33025': 6, '33026': 7, '33027': 8}

# Original labels (from the old data)
labels_old = ['32769', '33024', '32779', '32780', '33026', '32779', '32780', '33027',
              '32779', '32780', '33025', '32779', '32780', '33026', '32779', '32780', '33025']

# For this example, I assume you're extracting labels based on row indices (adjust based on actual event times)
# This should match the number of rows in your new datasets (1280 rows)
labels_new = []
for i in range(data1.shape[0]):
    event_idx = i % len(labels_old)  # Adjust this if the labels are not sequential
    labels_new.append(labels_old[event_idx])

# Convert list to a NumPy array
labels_new = np.array(labels_new)

# Now, we have the new data and corresponding labels (1280 rows x 8 columns, labels array with 1280 items)
# Data Channels Names
channels_name = ['Oz', 'O1', 'O2', 'PO3', 'POz', 'PO7', 'PO8', 'PO4']  # Example for 8 channels

# Convert data to microvolts (if necessary, depending on your data's scale)
data = 1e6 * data1  # Assuming data1, data2, data3 are the subsets

# Sampling frequency (adjust based on your data)
fs = 256  # Example, adjust as per your dataset's fs

# Placeholder for events (adjust according to your dataset's event times and values)
events = np.array([[44752, 0, 1], [45264, 0, 5], [45520, 0, 3]])  # Example events
event_ind = {'32769': 1, '32770': 2, '32779': 3, '32780': 4, '33024': 5,
             '33025': 6, '33026': 7, '33027': 8}  # Event mapping

# Print all the relevant information for the new data and labels
print(f"Data: {data.shape} \n")
print(f"Channels Name: {channels_name} \n")
print(f"Labels: {labels_new} \n")
print(f"Events: {events} \n")
print(f"Event Indices: {event_ind} \n")

import pandas as pd
# Create a DataFrame for the EEG data
df = pd.DataFrame(data, columns=channels_name)
# Add the labels to the DataFrame
df['Labels'] = labels_new
# Display the first few rows to visualize the table
print(df.head())

import numpy as np

# Load the .npy file data (assuming data is in shape [num_trials, num_samples, num_channels])
data1 = np.load('/content/drive/MyDrive/SSVEP-based-EEG-signal-processing-main/SSVEP-based-EEG-signal-processing-main/trials/13hz.npy')[:,:,0]  # 13 Hz trial data
data2 = np.load('/content/drive/MyDrive/SSVEP-based-EEG-signal-processing-main/SSVEP-based-EEG-signal-processing-main/trials/21hz.npy')[:,:,0]  # 21 Hz trial data
data3 = np.load('/content/drive/MyDrive/SSVEP-based-EEG-signal-processing-main/SSVEP-based-EEG-signal-processing-main/trials/17hz.npy')[:,:,0]  # 17 Hz trial data

# Example event information (adjust to match your real event timings and indices)
event_ind = {'32769': 1, '32770': 2, '32779': 3, '32780': 4, '33024': 5,
             '33025': 6, '33026': 7, '33027': 8}  # Define event labels (these map to specific event types)

# Original labels from old dataset (for reference, these will be assigned based on trials)
labels_old = ['32769', '33024', '32779', '32780', '33026', '32779', '32780', '33027',
              '32779', '32780', '33025', '32779', '32780', '33026', '32779', '32780', '33025']

# For this example, we map labels based on the total number of trials (adjust if events follow a different pattern)
labels_new = []

# Example: if we have 1280 trials per data set (change according to the real size of your data)
# Assign labels cyclically from the old labels for each subset of data (data1, data2, data3)
num_trials = data1.shape[0]  # Assuming data1 has 1280 trials
for i in range(num_trials):
    event_idx = i % len(labels_old)  # This will wrap around and assign labels cyclically
    labels_new.append(labels_old[event_idx])

# Convert list to NumPy array
labels_new = np.array(labels_new)

# Assuming data1, data2, data3 are 1280 trials x 8 channels (adjust based on your data)
# Collect all data into one array (e.g., for combined trials) for consistency
all_data = np.concatenate([data1, data2, data3], axis=0)  # Stack data vertically (along rows)

# Channels Names (example for 8 channels)
channels_name = ['Oz', 'O1', 'O2', 'PO3', 'POz', 'PO7', 'PO8', 'PO4']

# Define the sampling frequency (adjust as per your dataset's actual fs)
fs = 256  # Example, adjust based on your data's sample frequency

# Placeholder for event timings (replace with actual event times and values)
events = np.array([[44752, 0, 1], [45264, 0, 5], [45520, 0, 3]])  # Example event array
event_ind = {'32769': 1, '32770': 2, '32779': 3, '32780': 4, '33024': 5,
             '33025': 6, '33026': 7, '33027': 8}  # Map event indices

# Now, let's print the relevant information
print(f"Data Shape: {all_data_microvolts.shape} (trials x channels) \n")
print(f"Channels Names: {channels_name} \n")
print(f"Labels (for {num_trials * 3} trials): {labels_new} \n")
print(f"Events (example): {events} \n")
print(f"Event Indices: {event_ind} \n")

import pandas as pd
# Convert data1 into a DataFrame
df = pd.DataFrame(data1)
# Display the DataFrame
print(df)

print(data1.shape)
print(data2.shape)
print(data3.shape)

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
import scipy.signal as signal


# Generate the labels for each dataset (e.g., data1 = 13Hz, data2 = 21Hz, data3 = 17Hz)
y1 = np.full(data1.shape[0], 13)  # Label all data1 as 13Hz
y2 = np.full(data2.shape[0], 21)  # Label all data2 as 21Hz
y3 = np.full(data3.shape[0], 17)  # Label all data3 as 17Hz

# Combining data and labels
X = np.vstack([data1, data2, data3])
y = np.concatenate([y1, y2, y3])  #labels/frequencies

#Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Initialize and train a Random Forest classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)
# Predict on the test set
y_pred = clf.predict(X_test)

# Calculate the accuracy
#accuracy = accuracy_score(y_test, y_pred)
# predicted labels back tofrequency
label_to_freq = {
    '13': 13,
    '17': 17,
    '21': 21
}

# Mapping labels to frequencies
predicted_frequencies = [label_to_freq[str(label)] for label in y_pred]

# Output the predicted frequencies for each trial
print("\nPredicted Frequencies for Each Trial:")
for i, freq in enumerate(predicted_frequencies[:20]):  # first 20 predictions
    print(f"Trial {i + 1}: {freq} Hz")